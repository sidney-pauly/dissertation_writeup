% Mention that the faster we can run power flow the more data can be collected later -> a main goal
% is to make power flow as fast as possible
% Validation:
%   1. Comparision with results obtained by book
%   2. Comparision with vep results?
% Gauss-Seidel
% Newton-raphson
% python-c adapter
% sparse matricies optimization
% Show performance of different methods over grid size?

To solve the power flow equation \ref{eq:pf:full_pf_eq} a numerical solver
is required\autocite{power_system_analysis}. In the context of this analysis
a vast number switch states have to be explored. The more swtichstates
can be explored in the same amount of time the better. Thus, a fast power flow
solver is required. There are numerous solvers power flow with different speed
and versatility tradeoffs\autocite{pf_methods_comparison}. Some methods might
only work on specific grid topologies like radial topologies, some other methods
might require the network to have decoupled real and reactive power flows to work.
The Gauss-Seidel and Newton-Raphson methods are both comparatively robust and do not
have specific limitations on grid topology (meshed vs. radial)\autocite{pf_methods_comparison}.
Particularly the Newton-Raphson method is very fast for larger girds. Both methods
will be mathematically introduced and compared regarding their performance.

\subsection{Gauss-Seidel}

The Gauss-Seidel method works by iteratively approximating values
within a system of 
equations, such as the values of $V_i$ in case of the power 
flow \autoref{eq:pf:full_pf_eq}. Each subsequent approximation is
then used in the next iteration. There is no guarantee that
the method converges and its convergence is highly dependent on
the first guess being made. However, the approach
works well for most power flow problems\autocite{power_system_analysis}.\\
\\

To apply Gauss-Seidel the power flow equation (\ref{eq:pf:full_pf_eq}) has 
to be re-arranged for $V_i$ as such:

\begin{equation}
    V_i = \frac{\frac{S^*}{V_i^*} + \sum_{i \ne j}^N V_j y_{ij}}{y_{ii}}.
    \label{eq:pf:gs:start}
\end{equation}

As can be seen in \autoref{eq:pf:gs:start} the next guess for $V_i$
depends on the current value of $V_i$. Thus add a superscript $k$
to denote which iterative approximation of $V_i$ is meant:

\begin{equation}
    V_i^{(k+1)} = \frac{\frac{S^*}{{V_i^*}^{(k)}} + \sum_{i \ne j}^N V_j y_{ij}}{y_{ii}}.
    \label{eq:pf:gs}
\end{equation}

We can then apply a convergence criterion $\epsilon$ to be satisfied:

\begin{equation}
    |V_i^{(k+1)} - V_i^{(k)}| < \epsilon.
    \label{eq:pf:gs:convergance_criterion}
\end{equation}

We keep applying \autoref{eq:pf:gs} until \autoref{eq:pf:gs:convergance_criterion}
is satisfied.

To speed up the algorithm, an acceleration factor $\alpha$ can be applied:

\begin{equation}
    V_{i}^{(k+1)} = V_{i \ calc}^{(k + 1)} + \alpha(V_{i \ calc}^{(k + 1)} - V_{i}^{(k)}),
    \label{eq:pf:gs:acc}
\end{equation}

where $V_{i \ calc}$ is the original voltage calculated through \autoref{eq:pf:gs}.\\
Typically, a value of $1.3 < \alpha < 1.7$ is recommeded\autocite{power_system_analysis}.

\subsection{Newton-Raphson}

Another method to solve the power flow equations is
the Newton-Raphson method, which can be derived as follows.
Starting with any equation like:

\begin{equation}
    f(x) = c,
    \label{eq:pf:nr_start_point}
\end{equation}

we can now decompose $x$ into an initial estimate $x^{(0)}$ 
and the difference $\Delta x^{(0)}$ of that estimate to the correct solution, s.t.,
$x = x^{(0)} + \Delta x^{(0)}$. This yields:

\begin{equation}
    f(x^{(0)} + \Delta x^{(0)}) = c
\end{equation}

Taylor expanding this equation yields

\begin{equation}
    f(x^{(0)}) + (\frac{df}{dx})^{(0)} \Delta x^{(0)} + \frac{1}{2!} (\frac{d^2f}{dx^2})^{(0)} (\Delta x^{(0)})^2 + ... = c
\end{equation}

Assuming a small error $\Delta x^{(0)}$, the higher order terms may be dropped. Thus

\begin{equation}
    \Delta x^{(0)} \approxeq \frac{c - f(x^{(0)}) }{ (\frac{df}{dx})^{(0)} }
\end{equation}

This can now be used to obtain subsequent approximations. Namely,

\begin{align}
    \begin{split}
    x^{(1)}   &= x^{(0)} + \Delta x^{(0)},\\
    x^{(k+1)} &= x^{(k)} + \Delta x^{(k)},\\
    x^{(k+1)} &= x^{(k)} + \frac{c - f(x^{(k)}) }{ (\frac{df}{dx})^{(k)} }.
    \end{split}
    \label{eq:pf:nr_iteration}
\end{align}

\begin{figure}[H]
    \centering
    \input{drawings/newton_raphson.tex}
    \caption{
       Graphical representation of the Newton-Raphson method
    }
    \label{fig:pf:nr_graph}
\end{figure}

To make this approach work we need to arrange the power flow \autoref{eq:pf:full_pf_eq}
similar to \autoref{eq:pf:nr_start_point}. We can choose between making $c = I$ or 
$c = S^*$:

\begin{equation}
    \begin{split}
        \sum_{j = 1}^N V_i V_j y_{ij} = S^*_i\\
        \sum_{j = 1}^N V_j y_{ij} = I_i,
    \end{split}
    \label{eq:pf:nr_pf_starting_point}
\end{equation}

thus $f(x)$ becomes:

\begin{equation}
    \begin{split}
        f(x) &= S^*_i(V)\\
        f(x) &= I_i(V),
    \end{split}
    \label{eq:pf:nr:fx_def}
\end{equation}

To formulate Newton-Raphson we also need the derivates $\frac{dS^*_i(V)}{dV}$ or $\frac{dI_i(V)}{dV}$. As $V$ and $S$ are vectors
(one value for each node $i$), the full derivative is a Jacobian:

\begin{align}
    \frac{dS(V)}{dV} = J = 
    \begin{pmatrix}
        \frac{\partial S^*_1}{\partial V_1} & \frac{\partial S^*_1}{\partial V_2} & \dots  & \frac{\partial S^*_1}{\partial V_N}\\
        \frac{\partial S^*_2}{\partial V_1} & \ddots                              &        &                                    \\
        \vdots                              &                                     & \ddots &                                    \\
        \frac{\partial S^*_N}{\partial V_1} &                                     &        & \frac{\partial S^*_N}{\partial V_N}
    \end{pmatrix}.
\end{align}

As these are complex number there are an additional three choices, the derivative can either be taken
with respect to the Cartesian from, Polar form or be left as a complex derivative.\\
\\
All these different choices are mathematically different, but also have an impact
on computational and convergence
performance\autocite{newton_raphson_setup_choices}. Complex formulations
generally perform the worst, whilst Polar and
Cartesian formulations seem to have similar performance.
A current based setup performs slightly better in some 
scenarios\autocite{newton_raphson_setup_choices}.
However, due to the differences being minor, and a 
wider adoption of the polar power formulation, we will use this formulation here.\\

Reformulating eq. \ref{eq:pf:nr_pf_starting_point} in polar yields:

\begin{equation}
    \begin{split}
    P_i = \sum_{j=1}^N |V_i||V_j||y_{ij}| \cos(\theta_{ij} - \delta_i + \delta_j),\\
    Q_i = -\sum_{j=1}^N |V_i||V_j||y_{ij}| \sin(\theta_{ij} - \delta_i + \delta_j).
    \end{split}
    \label{eq:pf:nr_pf_pq}
\end{equation}

To obtain the derivative we can again produce a jacobian:

\begin{equation}
    J =  
    \begin{bmatrix}
        \vec{P}(|\vec{V}|, \ \vec{\delta})\\
        \vec{Q}(|\vec{V}|, \ \vec{\delta})
    \end{bmatrix}
    \begin{pmatrix}
        \frac{\partial}{\partial |\vec{\delta}|} & \frac{\partial}{\partial |\vec{V}|}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \frac{\partial \vec{P}}{\partial \vec{\delta}} & \frac{\partial \vec{P} }{\partial |\vec{V}|}\\
        \frac{\partial \vec{Q}}{\partial \vec{\delta}} & \frac{\partial \vec{Q} }{\partial |\vec{V}|}\\
    \end{pmatrix}
    =
    \begin{pmatrix}
        J_1 & J_2\\
        J_3 & J_4
    \end{pmatrix}.
    \label{eq:pf:nr_pf_jacobi}
\end{equation}

Note that all the values above are vectors, thus each of the entries $J_n$ expand into
their own sub Jacobians.\\

\autoref{eq:pf:nr:fx_def} and \autoref{eq:pf:nr_pf_jacobi} 
can be plugged into \autoref{eq:pf:nr_iteration} as $f(x)$ and $\frac{df}{dx}$
respectively:

\begin{equation}
    V^{(k+1)} = V^{(k)} + (P^{sch} - P^{(k)})J^{-1},
    \label{eq:pf:nr:preliminary}
\end{equation}

where $P^{sch}$ are the power values given as input constraints (used
as $c$) and $P^{(k)}$ the power values
calculated for this iteration.
$V$ and $P$ are row vectors as such:

\begin{equation}
    V = 
    \begin{pmatrix}
        \delta_1\\
        \dots\\
        \delta_N\\
        -\\
        \delta_1\\
        \dots\\
        \delta_N
    \end{pmatrix},
    \quad
    P = 
    \begin{pmatrix}
        P_1\\
        \dots\\
        P_N\\
        -\\
        Q_1\\
        \dots\\
        Q_N
    \end{pmatrix}.
\end{equation}

This also gives us our convergence criteria in the form of:

\begin{equation}
    \begin{split}
        \Delta P &= P^{sch} - P,\\
        |\Delta P| &< \epsilon.
    \end{split}
    \label{eq:pf:nr_pf_convergance}
\end{equation}

\autoref{eq:pf:nr:preliminary} can be re-written with $\Delta P$ as such:

\begin{equation}
    V^{(k+1)} = V^{(k)} + \Delta P \ J^{-1}.
    \label{eq:pf:nr_pf}
\end{equation}

Again \autoref{eq:pf:nr_pf} is applied until
\ref{eq:pf:nr_pf_convergance} is fulfilled.
